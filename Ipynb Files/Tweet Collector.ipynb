{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ky002\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ky002\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ky002\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ky002\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ky002\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Packages for data processing and machine learning\n",
    "import numpy as np, pandas as pd, sklearn as sk, torch as th \n",
    "# Packages for webpage crawling\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup as BS\n",
    "# Packages for nature language processing\n",
    "import spacy, pyinflect\n",
    "from pyinflect import getInflection, getAllInflections, getAllInflectionsOOV\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.util import ngrams as ng\n",
    "from nltk.tokenize import word_tokenize as tk\n",
    "from nltk.stem import WordNetLemmatizer as wn\n",
    "# Packages for Twitter API and configuration\n",
    "import tweepy as tw, configparser  \n",
    "# Packages about time\n",
    "import time as t, datetime as dt, rfc3339\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Car          Buy    Drive          Brand Brand Abbreviation  \\\n",
      "0             car       invest     rode           Ford                 VW   \n",
      "1            auto   purchasing   ridden            BMW           Mercedes   \n",
      "2      automobile     transact   riding     Volkswagen               Voho   \n",
      "3             bus     purchase  driving  Mercedes-Benz              Landy   \n",
      "4     convertible    acquiring      run           Audi             Bimmer   \n",
      "5            jeep      acquire     trip       Vauxhall                MBZ   \n",
      "6       limousine          buy     tour         Toyota                NaN   \n",
      "7         machine      bargain    drive            Kia                NaN   \n",
      "8           motor         shop     ride        Hyundai                NaN   \n",
      "9          pickup  acquisition    drove     Land Rover                NaN   \n",
      "10  station wagon          NaN      NaN            NaN                NaN   \n",
      "11          truck          NaN      NaN            NaN                NaN   \n",
      "12            van          NaN      NaN            NaN                NaN   \n",
      "13          wagon          NaN      NaN            NaN                NaN   \n",
      "14        vehicle          NaN      NaN            NaN                NaN   \n",
      "\n",
      "   Brand Model        Car Tool   Car Type  \n",
      "0       fiesta            grip      coupe  \n",
      "1      corolla          bumper  hatchback  \n",
      "2       Series            tyre      sedan  \n",
      "3         polo           brake     sports  \n",
      "4     sportage          bonnet        suv  \n",
      "5       tucson          airbag        NaN  \n",
      "6        corsa     carburettor        NaN  \n",
      "7      A-Class          piston        NaN  \n",
      "8    discovery          engine        NaN  \n",
      "9           A3         battery        NaN  \n",
      "10         NaN       fuel tank        NaN  \n",
      "11         NaN            hood        NaN  \n",
      "12         NaN  steering wheel        NaN  \n",
      "13         NaN     accelerator        NaN  \n",
      "14         NaN        seatbelt        NaN  \n",
      "889 (car OR auto OR automobile OR bus OR convertible OR jeep OR limousine OR machine OR motor OR pickup OR station OR wagon OR truck OR van OR wagon OR vehicle OR Ford OR BMW OR Volkswagen OR Mercedes-Benz OR Audi OR Vauxhall OR Toyota OR Kia OR Hyundai OR Land OR Rover OR VW OR Mercedes OR Voho OR Landy OR Bimmer OR MBZ OR fiesta OR corolla OR Series OR polo OR sportage OR tucson OR corsa OR A-Class OR discovery OR A3 OR grip OR bumper OR tyre OR brake OR bonnet OR airbag OR carburettor OR piston OR engine OR battery OR fuel OR tank OR hood OR steering OR wheel OR accelerator OR seatbelt OR coupe OR hatchback OR sedan OR sports OR suv) (invest OR purchasing OR transact OR purchase OR acquiring OR acquire OR buy OR bargain OR shop OR acquisition OR rode OR ridden OR riding OR driving OR run OR trip OR tour OR drive OR ride OR drove) lang:en place_country:GB -is:nullcast -has:links\n"
     ]
    }
   ],
   "source": [
    "# Reqest the webpages including the synonymsous words or meanings of these topics: car, buy, and drive\n",
    "car_url='https://www.thesaurus.com/browse/car'\n",
    "buy_url='https://www.thesaurus.com/browse/buy'\n",
    "buying_url='https://www.thesaurus.com/browse/buying'\n",
    "drive_url='https://www.thesaurus.com/browse/drive'\n",
    "\n",
    "# Crawling the synonyms of four topics: (car, buy, buying, drive) from Thesurus.com\n",
    "def keyword_extract(url_name,class_name):\n",
    "    page_name=r.get(url_name)\n",
    "    soup_name=BS(page_name.content,'html.parser')\n",
    "    key_soup=soup_name.find('ul', class_=class_name).find_all('a')\n",
    "    list_name=[]\n",
    "    for key in key_soup:\n",
    "        new_key=key['href'][8:].replace('%20',' ')\n",
    "        list_name.append(new_key)\n",
    "    return list_name\n",
    "\n",
    "# I only captured the red-marked words which contain the most closed meanings as the chosen topics\n",
    "car_sym=keyword_extract(car_url,'css-1xohnkh e1ccqdb60')[:14]\n",
    "car_sym.insert(0,'car')\n",
    "buy_sym=keyword_extract(buy_url,'css-wmtunb e1ccqdb60')[:4]\n",
    "buy_sym.insert(0,'buy')\n",
    "buying_sym=keyword_extract(buying_url,'css-1lj4erq e1ccqdb60')[:3]\n",
    "buying_sym.insert(0,'buying')\n",
    "buy_sym=buy_sym+buying_sym\n",
    "drive_sym=keyword_extract(drive_url,'css-n85ndd e1ccqdb60')[:4]\n",
    "drive_sym.insert(0,'drive')\n",
    "\n",
    "# Altough I have got thirty words as required keywords for tweet requests, it is recommended to include different tenses or forms of words \n",
    "#, as many Twitter users will use them depending on different contents.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "buy_str=' '.join(buy_sym)\n",
    "buy_doc=nlp(buy_str)\n",
    "buy_extension_list=[]\n",
    "for num in range(len(buy_doc)):\n",
    "    token = buy_doc[num]\n",
    "    if token.tag_ in ['NN','VB','VBG']:\n",
    "        buy_extension_list.append(token._.inflect('VB',inflect_oov=True))\n",
    "buy_extension_list=[ele for ele in list(set(buy_extension_list+['invest','shop','transact'])) if ele]\n",
    "\n",
    "def extension(sym):\n",
    "    sym_str=' '.join(sym)\n",
    "    sym_token=nlp(sym_str)\n",
    "    extension_list=[]\n",
    "    for num in range(len(sym_token)):\n",
    "        token = sym_token[num]\n",
    "        if token.tag_ in ['NN','VB','VBG']:\n",
    "            if str(token)!=token._.inflect('VBD',inflect_oov=True)[:len(token)]:\n",
    "                extension_list.append(token._.inflect('VBD',inflect_oov=True))              \n",
    "            if str(token)!=token._.inflect('VBG',inflect_oov=True)[:len(token)]:\n",
    "                 extension_list.append(token._.inflect('VBG',inflect_oov=True))\n",
    "            if str(token)!=token._.inflect('VBN',inflect_oov=True)[:len(token)]:\n",
    "                 extension_list.append(token._.inflect('VBN',inflect_oov=True))\n",
    "            if str(token)!=token._.inflect('VBZ',inflect_oov=True)[:len(token)]:\n",
    "                 extension_list.append(token._.inflect('VBZ',inflect_oov=True))\n",
    "    return extension_list\n",
    "\n",
    "buy_sym=list(set(extension(buy_extension_list)+buy_extension_list))\n",
    "buy_sym.append('acquisition')\n",
    "\n",
    "drive_sym=list(set(extension(drive_sym)+drive_sym))\n",
    "\n",
    "# According to car registration records of the UK from 2019 to 2022, the top 10 sales brands were selected as they might be mentioned more frequently than the other brands when talking about automative topics\n",
    "brand_list=['Ford','BMW','Volkswagen','Mercedes-Benz','Audi','Vauxhall','Toyota','Kia','Hyundai','Land Rover']\n",
    "\n",
    "#While searching for Google Trends data, various spellings or expressions that a user could use while posting about a car model were checked.\n",
    "brand_abb=['VW','Mercedes','Voho','Landy','Bimmer','MBZ']\n",
    "\n",
    "# The most popular Car sale Models in the UK, as a same replacement of the brand name: One model for one brand\n",
    "brand_model=['fiesta','corolla','Series','polo','sportage','tucson','corsa','A-Class','discovery','A3']\n",
    "\n",
    "# Name of car types when people are talking a series of car\n",
    "car_type=['coupe','hatchback','sedan','sports','suv']\n",
    "\n",
    "# Names of many important parts of car are also added in as I found that is a part of comments when people value their cars:15 most important words from https://www.collinsdictionary.com/word-lists/car-parts-of-a-car\n",
    "car_tool=['grip','bumper','tyre','brake','bonnet','airbag','carburettor','piston','engine','battery','fuel tank','hood','steering wheel','accelerator','seatbelt']\n",
    "\n",
    "#But there isn't such words like automobiling/automobiled or jeeping/jeeped, so it has to be deleted\n",
    "car_sym.remove('ride')\n",
    "car_sym.append('vehicle')\n",
    "\n",
    "def df_generator(name,list):\n",
    "    return pd.DataFrame({name:list})\n",
    "#Put all the words into a dataframe sorted by different segements.\n",
    "keywords=pd.concat([df_generator('Car',car_sym),\n",
    "                    df_generator('Buy',buy_sym),\n",
    "                    df_generator('Drive',drive_sym),\n",
    "                    df_generator('Brand',brand_list),\n",
    "                    df_generator('Brand Abbreviation',brand_abb),\n",
    "                    df_generator('Brand Model',brand_model), \n",
    "                    df_generator('Car Tool',car_tool),\n",
    "                    df_generator('Car Type',car_type)],\n",
    "                    axis=1)\n",
    "print(keywords)\n",
    "\n",
    "# To limit the search zone that has to contain the elements of car, purchase, drive \n",
    "keylist_of_car=car_sym+brand_list+brand_abb+brand_model+car_tool+car_type\n",
    "keylist_of_buy=buy_sym\n",
    "keylist_of_drive=drive_sym\n",
    "\n",
    "#Build the content of query: element 'car' is compulsory which the other elements are optional\n",
    "query_content='('+' '.join(keylist_of_car).replace(' ',' OR ')+') ('+' '.join(keylist_of_buy+keylist_of_drive).replace(' ',' OR ')+') lang:en place_country:GB -is:nullcast -has:links'\n",
    "print(len(query_content), query_content)\n",
    "# Build and read the config for password safety\n",
    "config=configparser.RawConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "api_key=config['twitter']['api_key']\n",
    "api_key_secret=config['twitter']['api_key_secret']\n",
    "access_token=config['twitter']['access_token']\n",
    "access_token_secret=config['twitter']['access_token_secret']\n",
    "bearer_token=config['twitter']['bearer_token']\n",
    "\n",
    "# Authtication of my Twitter api\n",
    "client = tw.Client(bearer_token,api_key, api_key_secret,access_token, access_token_secret,wait_on_rate_limit=True)\n",
    "\n",
    "# Building the timestrap of each hour\n",
    "def date_range(start_date, end_date):\n",
    "    while start_date <= end_date:\n",
    "        yield start_date\n",
    "        start_date+=dt.timedelta(hours=8)\n",
    "\n",
    "# Set the starting at the first hour of May 1st 2019 and the end time at the same hour of May 1st 2022\n",
    "start_date = datetime(2019, 5, 1, 0, 00,00)\n",
    "end_date = datetime(2022, 3, 1, 0, 00,00)\n",
    "# remember to change about the time\n",
    "first_time=[]\n",
    "second_time=[]\n",
    "for single_date in date_range(start_date, end_date):\n",
    "    first_time.append(single_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    second_time.append(single_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "first_time=first_time[:-1]\n",
    "second_time=second_time[1:]\n",
    "\n",
    "def rfc_time_convetor(time_list):\n",
    "    new_time_list=[]\n",
    "    for single_record in time_list:\n",
    "        datetime_object = datetime.strptime(single_record, \"%Y-%m-%d %H:%M:%S\")\n",
    "        rfc_records=rfc3339.rfc3339(datetime_object)\n",
    "        new_time_list.append(rfc_records)\n",
    "    return new_time_list\n",
    "\n",
    "rfc_first_time=rfc_time_convetor(first_time)\n",
    "rfc_second_time=rfc_time_convetor(second_time)\n",
    "\n",
    "# Retrieve all the required data from Twitter API from each eight hours per day of three years\n",
    "for start_time, end_time in zip(rfc_first_time,rfc_second_time):\n",
    "    tweet_info_small_list=[]\n",
    "    paginator=tw.Paginator(client.search_all_tweets,\n",
    "                            query_content,                            \n",
    "                            end_time=end_time,       \n",
    "                            start_time=start_time,\n",
    "                            tweet_fields = [\"created_at\", \"text\", \"lang\"],\n",
    "                            sort_order=['relevancy'],\n",
    "                            max_results=100).flatten(limit=250)\n",
    "    for tweet in paginator:\n",
    "        tweet_info_small_list.append(tweet.data)\n",
    "    tweets_datasource = pd.DataFrame(tweet_info_small_list)\n",
    "    tweets_datasource.to_csv('C:/Users/ky002/Desktop/Dickens/Postgraduate/Dissertation/Data Source/twitter_full_version_data.csv',sep=',', mode='a',encoding='utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ky002\\Desktop\\Dickens\\Postgraduate\\Dissertation\\Data Source\\TwRes_model1.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ky002/Desktop/Dickens/Postgraduate/Dissertation/Data%20Source/TwRes_model1.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39m#Get rid of the empty lines and \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ky002/Desktop/Dickens/Postgraduate/Dissertation/Data%20Source/TwRes_model1.ipynb#ch0000000?line=1'>2</a>\u001b[0m raw_data\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mTwData_100.csv\u001b[39m\u001b[39m'\u001b[39m,sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m,header\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf_8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ky002/Desktop/Dickens/Postgraduate/Dissertation/Data%20Source/TwRes_model1.ipynb#ch0000000?line=2'>3</a>\u001b[0m raw_data\u001b[39m=\u001b[39mraw_data\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mUnnamed: 5\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ky002/Desktop/Dickens/Postgraduate/Dissertation/Data%20Source/TwRes_model1.ipynb#ch0000000?line=3'>4</a>\u001b[0m raw_data\u001b[39m=\u001b[39mraw_data[raw_data[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m!=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcreated_at\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#Get rid of the empty lines and \n",
    "raw_data=pd.read_csv('TwData_100.csv',sep=',',header=0,encoding='utf_8')\n",
    "raw_data=raw_data.drop(columns=['Unnamed: 0','Unnamed: 5'])\n",
    "raw_data=raw_data[raw_data['id']!='id'].sort_values(by='created_at').reset_index(drop=True)\n",
    "\n",
    "# A forbidden word dictionary for meaningless words\n",
    "exclude_words=sw.words('english')\n",
    "print(exclude_words)\n",
    "exclude_words=exclude_words+['@','|','/','\\'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107471"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46c53dfe207e1bf458957234df624eec1401b1cc104da5f2e9e6134dc26f11c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
